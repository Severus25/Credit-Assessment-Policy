import json
import logging
from collections import defaultdict
from config import settings # <-- Use our centralized settings

class MergerAgent:
    """
    An agent that simulates the first two stages of the workflow.
    It takes the individual outputs from the Exposure Aggregator and
    Risk Scoring agents and merges them into a single, unified log file
    that downstream agents can process.
    """
    def __init__(self, agent_id="MergerAgent01"):
        self.agent_id = agent_id
        self.logger = logging.getLogger(self.agent_id)

        # Define file paths using the settings module
        self.exposure_file = settings.EXPOSURE_REPORT_FILE
        self.risk_file = settings.RISK_SCORE_FILE # This will point to Risk_score_output.json
        self.limit_file = settings.OUTPUT_FILE # This file is an input for the final merge
        self.output_file = settings.UNIFIED_LOG_FILE

    def _load_json(self, file_path):
        """Loads a JSON file with error handling."""
        try:
            with open(file_path, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.warning(f"Data file not found: {file_path}. This agent's data will be skipped.")
            return []
        except json.JSONDecodeError:
            self.logger.error(f"Failed to decode JSON from {file_path}.")
            return []

    def run(self):
        """Main execution method to generate the unified log."""
        self.logger.info("--- Agent execution started ---")
        self.logger.info("Simulating upstream agents by merging their outputs...")
        
        exposure_data = self._load_json(self.exposure_file)
        risk_data = self._load_json(self.risk_file)
        # The limit_file is generated by the LimitSetter, so we load it here
        # to create the FINAL version of the unified log.
        limit_data = self._load_json(self.limit_file)

        workflows = defaultdict(list)

        # Process data from all three agents
        all_data = exposure_data + risk_data + limit_data

        for record in all_data:
            customer_id = record.get("customer_id")
            if customer_id:
                workflows[customer_id].append(record)

        # Build the final unified log structure
        unified_log = {"workflows": []}
        for customer_id, logs in workflows.items():
            unified_log["workflows"].append({
                "workflow_id": f"WF_{customer_id}",
                "customer_id": customer_id,
                "logs": sorted(logs, key=lambda x: x.get("timestamp", ""))
            })

        try:
            with open(self.output_file, "w") as f:
                json.dump(unified_log, f, indent=2)
            self.logger.info(f"Action successful. Unified log file created/updated at '{self.output_file}'.")
        except Exception as e:
            self.logger.error(f"Failed to write unified log file: {e}")
            
        self.logger.info("--- Agent execution finished ---")